{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row-wise processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3847875, 265)\n"
     ]
    }
   ],
   "source": [
    "#merging and normalizing outputs of the PCL to the AE format\n",
    "#PCL -> AE\n",
    "def load_and_normalize(folder_path):\n",
    "    all_data = []\n",
    "    min_val, max_val = -10, 7.5\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".npy\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            data = np.load(file_path)\n",
    "            \n",
    "            # Normalize each line\n",
    "            for line in data:\n",
    "                \n",
    "                line = np.maximum(line, min_val)\n",
    "                line = np.minimum(line, max_val)\n",
    "                line = (line - min_val) / (max_val - min_val)\n",
    "                all_data.append(line)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_data = np.vstack(all_data)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "folder_path = '/home/carol/NaroNet-main/NaroNet-main/Endometrial_POLE/Patch_Contrastive_Learning/Image_Patch_Representation'\n",
    "allembs = load_and_normalize(folder_path)\n",
    "print(np.shape(allembs))\n",
    "np.save('ConcInp.npy', allembs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting and renormalzie AE output to the Naronet format\n",
    "#AE->Naronet\n",
    "init_embs_path = \"/home/carol/NaroNet-main/NaroNet-main/Endometrial_POLE/Patch_Contrastive_Learning/Image_Patch_Representation\"\n",
    "rec_path = \"/home/carol/NaroNet-main/EXP3/IDEC-toy/results/embs/reconstructed_x.npy\"\n",
    "out_path = \"/home/carol/NaroNet-main/EXP3/Data/Split reconstruction\"\n",
    "reconstructed_embeddings = np.load(rec_path)\n",
    "reconstructed_embeddings = reconstructed_embeddings * (7.5 - (-10)) + (-10) #rescale to the original value range\n",
    "current_position= 0\n",
    "for file_name in os.listdir(init_embs_path):\n",
    "    file_path = os.path.join(init_embs_path, file_name)\n",
    "    cr_emb = np.load(file_path)  \n",
    "    line_count = cr_emb.shape[0]\n",
    "    \n",
    "    extracted_embeddings = reconstructed_embeddings[current_position : current_position + line_count]\n",
    "    current_position += line_count\n",
    "    new_file_path = os.path.join(out_path, file_name)\n",
    "    np.save(new_file_path, extracted_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column-wise processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3847875, 265)\n"
     ]
    }
   ],
   "source": [
    "#merging outputs of the PCL to the AE format\n",
    "#PCL -> AE\n",
    "def load(folder_path):\n",
    "    all_data = []\n",
    "    min_val, max_val = -10, 7.5\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".npy\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            data = np.load(file_path)\n",
    "\n",
    "\n",
    "            for line in data:\n",
    "                all_data.append(line)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_data = np.vstack(all_data)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "folder_path = '/home/carol/NaroNet-main/NaroNet-main/Endometrial_POLE/Patch_Contrastive_Learning/Image_Patch_Representation'\n",
    "allembs = load(folder_path)\n",
    "print(np.shape(allembs))\n",
    "np.save('ConcInp_noNorm.npy', allembs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(data):\n",
    "    # Assuming `data` is your (300k, 256) NumPy array\n",
    "    data_min = data.min(axis=0)  # Minimum value of each column\n",
    "    data_max = data.max(axis=0)  # Maximum value of each column\n",
    "\n",
    "    # Min-Max normalization to scale data to the [0, 1] range\n",
    "    data_normalized = (data - data_min) / (data_max - data_min)\n",
    "\n",
    "    # Handle potential divisions by zero if a column is constant\n",
    "    data_normalized = np.nan_to_num(data_normalized, nan=0.0)\n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/carol/NaroNet-main/EXP3/ConcInp_noNorm.npy\")\n",
    "normal_data  = normal(data)\n",
    "np.save('ConcInpCol.npy', normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_col(data_normalized):\n",
    "    # Assuming `data_normalized` is your normalized data array\n",
    "    # Define min and max values for the rescaling process\n",
    "    min_val_first_two = -10\n",
    "    max_val_first_two = 1750\n",
    "    min_val_rest = -10\n",
    "    max_val_rest = 7.5\n",
    "\n",
    "    # Initialize an array to hold the rescaled values, same shape as `data_normalized`\n",
    "    data_rescaled = np.zeros_like(data_normalized)\n",
    "\n",
    "    # Rescale the first two columns\n",
    "    for i in range(2):\n",
    "        data_rescaled[:, i] = data_normalized[:, i] * (max_val_first_two - min_val_first_two) + min_val_first_two\n",
    "        # Cap values at 1750 for the first two columns\n",
    "        data_rescaled[:, i] = np.where(data_rescaled[:, i] > max_val_first_two, max_val_first_two, data_rescaled[:, i])\n",
    "\n",
    "    # Rescale the rest of the columns\n",
    "    for i in range(2, data_normalized.shape[1]):\n",
    "        data_rescaled[:, i] = data_normalized[:, i] * (max_val_rest - min_val_rest) + min_val_rest\n",
    "        # No specific capping mentioned for these, but you could apply similar logic if needed\n",
    "\n",
    "    # Now `data_rescaled` contains the rescaled values, with the first two columns capped at 1750\n",
    "    return data_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting and renormalzie AE output to the Naronet format\n",
    "#AE->Naronet\n",
    "#Image patch Rep needs to be non-empty\n",
    "init_embs_path = \"/home/carol/NaroNet-main/NaroNet-main/Endometrial_POLE/Patch_Contrastive_Learning/Image_Patch_Representation\"\n",
    "rec_path = \"/home/carol/NaroNet-main/EXP3/IDEC-toy/results/embs/reconstructed_x.npy\"\n",
    "out_path = \"/home/carol/NaroNet-main/EXP3/Data/Split reconstruction\"\n",
    "reconstructed_embeddings = np.load(rec_path)\n",
    "reconstructed_embeddings = rescale_col(reconstructed_embeddings) #rescale to the original value range\n",
    "current_position= 0\n",
    "for file_name in os.listdir(init_embs_path):\n",
    "    file_path = os.path.join(init_embs_path, file_name)\n",
    "    cr_emb = np.load(file_path)  \n",
    "    line_count = cr_emb.shape[0]\n",
    "    \n",
    "    extracted_embeddings = reconstructed_embeddings[current_position : current_position + line_count]\n",
    "    current_position += line_count\n",
    "    new_file_path = os.path.join(out_path, file_name)\n",
    "    np.save(new_file_path, extracted_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_labels(path1, path2):\n",
    "    # Load the Excel files\n",
    "    data1 = pd.read_excel(path1, usecols=[1, 3], header=None, names=['Subject_name', 'Label'])\n",
    "    data2 = pd.read_excel(path2, usecols=[1, 3], header=None, names=['Subject_name', 'Label'])\n",
    "    \n",
    "    # Merge the two datasets on 'Subject_name'\n",
    "    merged_data = pd.merge(data1, data2, on='Subject_name', suffixes=('_1', '_2'))\n",
    "    \n",
    "    # Find mismatches in 'Label'\n",
    "    mismatches = merged_data[merged_data['Label_1'] != merged_data['Label_2']]\n",
    "    \n",
    "    # Print the subject names with differing labels\n",
    "    for subject in mismatches['Subject_name']:\n",
    "        print(subject)\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "# compare_labels('path_to_file1.xlsx', 'path_to_file2.xlsx')\n",
    "path1 = \"/home/carol/NaroNet-main/Datasets/Results/11_Exp2_Repeat1/NaroNet/Response/Cross_validation_results/Prediction_values_Response_Fold13.xlsx\"\n",
    "path2 = \"/home/carol/NaroNet-main/Datasets/Results/10_Exp2_CorFolds/NaroNet/Response/Cross_validation_results/Prediction_values_Response_Fold13.xlsx\"\n",
    "compare_labels(path1,path2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anotherone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
